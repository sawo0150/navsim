import gzip
import logging
import os
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import torch
from tqdm import tqdm

from navsim.common.dataloader import SceneLoader
from navsim.planning.training.abstract_feature_target_builder import AbstractFeatureBuilder, AbstractTargetBuilder

logger = logging.getLogger(__name__)


def load_feature_target_from_pickle(path: Path) -> Dict[str, torch.Tensor]:
    """Helper function to load pickled feature/target from path."""
    with gzip.open(path, "rb") as f:
        data_dict: Dict[str, torch.Tensor] = pickle.load(f)
    return data_dict


def dump_feature_target_to_pickle(path: Path, data_dict: Dict[str, torch.Tensor]) -> None:
    """Helper function to save feature/target to pickle."""
    # Use compresslevel = 1 to compress the size but also has fast write and read.
    with gzip.open(path, "wb", compresslevel=1) as f:
        pickle.dump(data_dict, f)


class CacheOnlyDataset(torch.utils.data.Dataset):
    """Dataset wrapper for feature/target datasets from cache only."""

    def __init__(
        self,
        cache_path: str,
        feature_builders: List[AbstractFeatureBuilder],
        target_builders: List[AbstractTargetBuilder],
        log_names: Optional[List[str]] = None,
    ):
        """
        Initializes the dataset module.
        :param cache_path: directory to cache folder
        :param feature_builders: list of feature builders
        :param target_builders: list of target builders
        :param log_names: optional list of log folder to consider, defaults to None
        """
        super().__init__()
        assert Path(cache_path).is_dir(), f"Cache path {cache_path} does not exist!" 
        # cache_pathê°€ ìœ íš¨í•œ ë””ë ‰í† ë¦¬ì¸ì§€ í™•ì¸ - ì•„ë‹ˆë©´ assert ì‹¤íŒ¨ë¡œ í”„ë¡œê·¸ë¨ ë©ˆì¶¤
        self._cache_path = Path(cache_path)

        if log_names is not None:
            self.log_names = [Path(log_name) for log_name in log_names if (self._cache_path / log_name).is_dir()]
        else:
            self.log_names = [log_name for log_name in self._cache_path.iterdir()]
        # log_namesê°€ ì£¼ì–´ì§€ë©´ â†’ ê·¸ ì¤‘ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë¡œê·¸ ë””ë ‰í† ë¦¬ë§Œ ì‚¬ìš©
        # ì—†ìœ¼ë©´ â†’ ìºì‹œ ê²½ë¡œ ì•ˆì˜ ëª¨ë“  ë¡œê·¸ í´ë”ë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©

        self._feature_builders = feature_builders
        self._target_builders = target_builders
        # builderë“¤ì€ .gz íŒŒì¼ëª…ê³¼ ë‚´ë¶€ í¬ë§· ì •ì˜ì— ì“°ì„

        self._valid_cache_paths: Dict[str, Path] = self._load_valid_caches(
            cache_path=self._cache_path,
            feature_builders=self._feature_builders,
            target_builders=self._target_builders,
            log_names=self.log_names,
        )
        # _load_valid_caches()ë¥¼ í˜¸ì¶œí•´ì„œ ì‹¤ì œ .gzê°€ ë‹¤ ì¡´ì¬í•˜ëŠ” tokenë§Œ ìˆ˜ì§‘

        self.tokens = list(self._valid_cache_paths.keys())

    def __len__(self) -> int:
        """
        :return: number of samples to load
        """
        return len(self.tokens)

    def __getitem__(self, idx: int) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:
        """
        Loads and returns pair of feature and target dict from data.
        :param idx: index of sample to load.
        :return: tuple of feature and target dictionary
        """
        return self._load_scene_with_token(self.tokens[idx])

    @staticmethod
    def _load_valid_caches(
        cache_path: Path,
        feature_builders: List[AbstractFeatureBuilder],
        target_builders: List[AbstractTargetBuilder],
        log_names: List[Path],
    ) -> Dict[str, Path]:
        """
        Helper method to load valid cache paths.
        :param cache_path: directory of training cache folder
        :param feature_builders: list of feature builders
        :param target_builders: list of target builders
        :param log_names: list of log paths to load
        :return: dictionary of tokens and sample paths as keys / values
        """
        # ì£¼ì–´ì§„ ë¡œê·¸ ì´ë¦„ ëª©ë¡ì„ ìˆœíšŒí•˜ë©°
        # ê° log ë””ë ‰í† ë¦¬ ì•„ë˜ì˜ token í´ë”ë¥¼ íƒìƒ‰í•¨
        valid_cache_paths: Dict[str, Path] = {}

        for log_name in tqdm(log_names, desc="Loading Valid Caches"):
            log_path = cache_path / log_name
            for token_path in log_path.iterdir():
            # log í´ë” ë‚´ì—ì„œ ê° token í´ë” í™•ì¸ (ì˜ˆ: token1234)
                found_caches: List[bool] = []
                for builder in feature_builders + target_builders:
                    data_dict_path = token_path / (builder.get_unique_name() + ".gz")
                    found_caches.append(data_dict_path.is_file())
                # ê° builder ì´ë¦„ì— í•´ë‹¹í•˜ëŠ” .gz íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if all(found_caches):
                    valid_cache_paths[token_path.name] = token_path
                # ëª¨ë“  .gzê°€ ë‹¤ ìˆìœ¼ë©´ â†’ ìœ íš¨í•œ ìºì‹œë¡œ ì¸ì • â†’ valid_cache_pathsì— ë“±ë¡

        return valid_cache_paths

    def _load_scene_with_token(self, token: str) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:
        """
        Helper method to load sample tensors given token
        :param token: unique string identifier of sample
        :return: tuple of feature and target dictionaries
        """
        # _cache_scene_with_token ì—­ê³¼ì •

        token_path = self._valid_cache_paths[token]

        features: Dict[str, torch.Tensor] = {}
        for builder in self._feature_builders:
            data_dict_path = token_path / (builder.get_unique_name() + ".gz")
            data_dict = load_feature_target_from_pickle(data_dict_path)
            features.update(data_dict)
            # ê° builderì— ëŒ€ì‘í•˜ëŠ” .gz íŒŒì¼ì—ì„œ feature tensorë“¤ì„ ë¶ˆëŸ¬ì™€ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€

        targets: Dict[str, torch.Tensor] = {}
        for builder in self._target_builders:
            data_dict_path = token_path / (builder.get_unique_name() + ".gz")
            data_dict = load_feature_target_from_pickle(data_dict_path)
            targets.update(data_dict)
            # target builderë“¤ë„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬

        return (features, targets)

# SceneLoader	ë¡œê·¸(.pkl)ë¥¼ ì½ê³ , scene ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ tokenë³„ë¡œ sceneì„ ê´€ë¦¬
# Dataset	SceneLoaderê°€ ì œê³µí•œ tokenìœ¼ë¡œë¶€í„° feature/targetì„ ìƒì„±í•˜ê³ , í•™ìŠµ ë°ì´í„°ë¡œ ì œê³µ
# cache_path	feature/targetì„ ë¯¸ë¦¬ ì €ì¥í•´ë‘ëŠ” ë””ë ‰í† ë¦¬
# feature_builders, target_builders	ê° sceneì—ì„œ í•™ìŠµì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ëª¨ë“ˆë“¤
class Dataset(torch.utils.data.Dataset):
    def __init__(
        self,
        scene_loader: SceneLoader,
        feature_builders: List[AbstractFeatureBuilder],
        target_builders: List[AbstractTargetBuilder],
        cache_path: Optional[str] = None,
        force_cache_computation: bool = False,
    ):
        super().__init__()
        self._scene_loader = scene_loader
        self._feature_builders = feature_builders
        self._target_builders = target_builders

        self._cache_path: Optional[Path] = Path(cache_path) if cache_path else None
        self._force_cache_computation = force_cache_computation
        self._valid_cache_paths: Dict[str, Path] = self._load_valid_caches( # Data ë¶ˆëŸ¬ì˜¤ëŠ”ë²•1 - cache ë¶ˆëŸ¬ì˜¤ê¸°
            self._cache_path, feature_builders, target_builders
        )

        if self._cache_path is not None:
            self.cache_dataset() # Data ë¶ˆëŸ¬ì˜¤ëŠ”ë²•2 - cache ìƒì„±

    @staticmethod
    # @staticmethodë€ëŠ” í´ë˜ìŠ¤ ë‚´ë¶€ì— ìˆì§€ë§Œ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤(self)ì— ì ‘ê·¼í•˜ì§€ ì•ŠëŠ” ë©”ì„œë“œ
    def _load_valid_caches(
        cache_path: Optional[Path],
        feature_builders: List[AbstractFeatureBuilder],
        target_builders: List[AbstractTargetBuilder],
    ) -> Dict[str, Path]:
        """
        Helper method to load valid cache paths.
        :param cache_path: directory of training cache folder
        :param feature_builders: list of feature builders
        :param target_builders: list of target builders
        :return: dictionary of tokens and sample paths as keys / values
        """
        # ì´ í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ìºì‹œ ê²½ë¡œ ì•ˆì— ìˆëŠ” íŒŒì¼ë“¤ì„ í™•ì¸í•´ì„œ,
        # ëª¨ë“  featureì™€ targetì— í•´ë‹¹í•˜ëŠ” .gz íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ ,
        # ìˆìœ¼ë©´ _valid_cache_paths[token] = í•´ë‹¹ ê²½ë¡œë¡œ ì¶”ê°€í•¨.

        valid_cache_paths: Dict[str, Path] = {}

        if (cache_path is not None) and cache_path.is_dir():   # ìºì‹œ ê²½ë¡œê°€ ìˆê³  ë””ë ‰í† ë¦¬ì¼ ê²½ìš°ì—ë§Œ ì§„í–‰
            for log_path in cache_path.iterdir():   # ë¡œê·¸ ë‹¨ìœ„ë¡œ ìˆœíšŒ
                for token_path in log_path.iterdir(): # ë¡œê·¸ í´ë” ì•ˆì˜ í† í°ë³„ í´ë”ë¥¼ ìˆœíšŒ
                    found_caches: List[bool] = [] # "ê° builderê°€ ìš”êµ¬í•˜ëŠ” .gz íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€"ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
                    for builder in feature_builders + target_builders: # feature_buildersì™€ target_buildersë¥¼ í•©ì³ì„œ í•˜ë‚˜ì”© ë°˜ë³µ
                        data_dict_path = token_path / (builder.get_unique_name() + ".gz")
                        found_caches.append(data_dict_path.is_file())   # íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                    if all(found_caches):   # ëª¨ë‘ ì¡´ì¬í•´ì•¼ ìœ íš¨í•œ ìºì‹œ - found_cachesê°€ [True, True, True]ë¼ë©´ â†’ all()ì€ True
                        valid_cache_paths[token_path.name] = token_path

        return valid_cache_paths

    def _cache_scene_with_token(self, token: str) -> None:
        """
        Helper function to compute feature / targets and save in cache.
        :param token: unique identifier of scene to cache
        """

        scene = self._scene_loader.get_scene_from_token(token)
        # scene_loaderì—ì„œ tokenì— í•´ë‹¹í•˜ëŠ” sceneì„ ê°€ì ¸ì˜¤ê³ 
        agent_input = scene.get_agent_input()   #return AgentInput(ego_statuses, cameras, lidars)
        #  AgentInputì€ í•™ìŠµ featureë¥¼ ìƒì„±í•  ë•Œ í•„ìš”í•œ ìµœì†Œ ë‹¨ìœ„ (Egoì˜ pose, camera, lidar ë°ì´í„°).
        metadata = scene.scene_metadata
        token_path = self._cache_path / metadata.log_name / metadata.initial_token
        # SceneMetadataì—ì„œ ì´ sceneì´ ì–´ëŠ ë¡œê·¸(log_name) ì†Œì†ì¸ì§€, í† í°ì€ ë­”ì§€ ì•Œì•„ëƒ„.
        # ë””ë ‰í† ë¦¬ êµ¬ì¡°: cache_path / log_name / token

        os.makedirs(token_path, exist_ok=True)

        for builder in self._feature_builders:
            data_dict_path = token_path / (builder.get_unique_name() + ".gz")
            data_dict = builder.compute_features(agent_input)   # Dict[str, Tensor]ê°€ returnë¨
            dump_feature_target_to_pickle(data_dict_path, data_dict) # dictë¥¼ .gz íŒŒì¼ë¡œ ì €ì¥
            # featureë¥¼ ë§Œë“œëŠ” builderë“¤ì„ í•˜ë‚˜ì”© ìˆœíšŒí•˜ë©´ì„œ,
            # compute_features(agent_input) í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ ê³„ì‚°
            # ê·¸ ê²°ê³¼ë¥¼ .gz íŒŒì¼ë¡œ ì €ì¥

        for builder in self._target_builders:
            data_dict_path = token_path / (builder.get_unique_name() + ".gz")
            data_dict = builder.compute_targets(scene)
            dump_feature_target_to_pickle(data_dict_path, data_dict)

        self._valid_cache_paths[token] = token_path

    def _load_scene_with_token(self, token: str) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:
        """
        Helper function to load feature / targets from cache.
        :param token:  unique identifier of scene to load
        :return: tuple of feature and target dictionaries
        """
        # _cache_scene_with_token ì—­ê³¼ì •

        token_path = self._valid_cache_paths[token]

        features: Dict[str, torch.Tensor] = {}
        for builder in self._feature_builders:
            data_dict_path = token_path / (builder.get_unique_name() + ".gz")
            data_dict = load_feature_target_from_pickle(data_dict_path)
            features.update(data_dict)

        targets: Dict[str, torch.Tensor] = {}
        for builder in self._target_builders:
            data_dict_path = token_path / (builder.get_unique_name() + ".gz")
            data_dict = load_feature_target_from_pickle(data_dict_path)
            targets.update(data_dict)

        return (features, targets)

    def cache_dataset(self) -> None:
        """Caches complete dataset into cache folder."""
        # ìºì‹œê°€ ì—†ì„ ê²½ìš° ëª¨ë“  sceneì„ ì§ì ‘ ê³„ì‚°í•´ì„œ .gz íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì—­í• 

        assert self._cache_path is not None, "Dataset did not receive a cache path!"
        os.makedirs(self._cache_path, exist_ok=True)

        # determine tokens to cache
        if self._force_cache_computation:
            tokens_to_cache = self._scene_loader.tokens
        # ë¬´ì¡°ê±´ ìƒˆë¡œ ìºì‹± (force_cache_computation=True) - ìºì‹œê°€ ì´ë¯¸ ìˆë”ë¼ë„ ë¬´ì‹œí•˜ê³  ì „ë¶€ ë‹¤ì‹œ ê³„ì‚°í•¨.
        else:
            tokens_to_cache = set(self._scene_loader.tokens) - set(self._valid_cache_paths.keys())
            # SceneLoaderê°€ ê°€ì§„ ì „ì²´ í† í° ëª©ë¡ ì¤‘ì—ì„œ ì•„ì§ _valid_cache_pathsì— ë“±ë¡ë˜ì§€ ì•Šì€ í† í°ë§Œ ê³¨ë¼ëƒ„.
            tokens_to_cache = list(tokens_to_cache)
            logger.info(
                f"""
                Starting caching of {len(tokens_to_cache)} tokens.
                Note: Caching tokens within the training loader is slow. Only use it with a small number of tokens.
                You can cache large numbers of tokens using the `run_dataset_caching.py` python script.
                """
                # ìºì‹± ê°œìˆ˜ê°€ ë§ì„ ë•Œ ê²½ê³  ë©”ì‹œì§€ë¥¼ ì•Œë ¤ì¤Œ. í•™ìŠµ ì¤‘ì— ìºì‹±í•˜ë ¤ í•˜ì§€ ë§ê³ , ë”°ë¡œ ìºì‹± ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ë¼ëŠ” ê¶Œì¥ ì‚¬í•­.
            )

        for token in tqdm(tokens_to_cache, desc="Caching Dataset"):
            self._cache_scene_with_token(token)
            # ê°ê°ì˜ tokenì— ëŒ€í•´ _cache_scene_with_token(token) í˜¸ì¶œ

    def __len__(self) -> None:
        """
        :return: number of samples to load
        """
        return len(self._scene_loader)
        # ì „ì²´ ë°ì´í„°ì…‹ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•¨. ì¦‰, ëª‡ ê°œì˜ sample(token)ì´ ìˆëŠ”ì§€ë¥¼ ì•Œë ¤ì¤˜.
        # self._scene_loader.tokens ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ì™€ ê°™ì•„.
        # í•™ìŠµ ì¤‘ì— PyTorchê°€ ë°°ì¹˜ ê°œìˆ˜ë¥¼ ê³„ì‚°í•  ë•Œ ì´ê±¸ ê¸°ë°˜ìœ¼ë¡œ ì‚¼ìŒ.

    def __getitem__(self, idx: int) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:
        """
        Get features or targets either from cache or computed on-the-fly.
        :param idx: index of sample to load.
        :return: tuple of feature and target dictionary
        """

        token = self._scene_loader.tokens[idx]
        # í˜„ì¬ ì¸ë±ìŠ¤(idx)ì— í•´ë‹¹í•˜ëŠ” sceneì˜ tokenì„ ê°€ì ¸ì˜¤ê¸°
        features: Dict[str, torch.Tensor] = {}
        targets: Dict[str, torch.Tensor] = {}

        if self._cache_path is not None:
            assert (
                token in self._valid_cache_paths.keys()
            ), f"The token {token} has not been cached yet, please call cache_dataset first!"
            # cache_path=Noneìœ¼ë¡œ Datasetì„ ë§Œë“¤ë©´:
            # ìºì‹œ ë¡œë”©ì€ ì•ˆ ë˜ê³ ,
            # __getitem__()ì—ì„œ ì‹¤ì‹œê°„ ê³„ì‚°ìœ¼ë¡œ ì²˜ë¦¬í•¨.
            # í•˜ì§€ë§Œ cache_dataset()ì„ ë¶€ë¥´ë©´ assertì— ê±¸ë ¤ì„œ ğŸ’¥ë©ˆì¶°ë²„ë¦¼.
            features, targets = self._load_scene_with_token(token)
        else:
            scene = self._scene_loader.get_scene_from_token(self._scene_loader.tokens[idx])
            agent_input = scene.get_agent_input()
            for builder in self._feature_builders:
                features.update(builder.compute_features(agent_input))
            for builder in self._target_builders:
                targets.update(builder.compute_targets(scene))

        return (features, targets)
